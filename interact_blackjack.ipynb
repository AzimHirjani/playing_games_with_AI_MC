{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Black Jack Open AI Gym</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this lab we will review the openai gym implementation of Black jack. You will interact with the AI Gym Black Jack environment, see how a random policy performs and play against the environment your self.</p>\n",
    "<p>Blackjack is a card game where the goal is to obtain cards that sum to the near as possible to 21 without going over.  They're playing against a fixed dealer. Face cards (Jack, Queen, King) have point value 10. Aces can either count as 11 or 1, and it's called 'usable' at 11. This game is played with an infinite deck (or with replacement). The game starts with dealer having one face up and one face down card, while player having two face up cards. (Virtually for all Blackjack games today). The player can request additional cards (hit=1) until they decide to stop (stick=0) or exceed 21 (bust). After the player sticks, the dealer reveals their facedown card, and draws until their sum is 17 or greater.  If the dealer goes bust the player wins. If neither player nor dealer busts, the outcome (win, lose, draw) is decided by whose sum is closer to 21.  The reward for winning is +1,drawing is 0, and losing is -1 [1].</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, you will play Blackjack environment on open AI gym </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#S\">Sampling</a></li>\n",
    "    <li><a href=\"#RP\">Play BlackJack With Random Policy  </a></li>\n",
    "    <li><a href=\"#YPE\">You Play Environment </a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you need to install open  ai gym  uncomment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module will have some useful functions to help you understand open ai gym's blackjack environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blackjackutility import get_total,game_result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<ul>\n",
    "    <li><code>get_total</code> will calculate the maximum total of a hand in 21. It will also take into consideration if the Deck has aces</li>\n",
    "    <li><code>game_result </code>this function will determine the results of a game of Black Jack after an episode </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"S\">Sampling</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an open ai blackjack gym environment  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment= gym.make('Blackjack-v0')\n",
    "# Source code at \n",
    "# https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation of is a 3-tuple of: the players (you) current sum, the dealer's one showing card (1-10 where 1 is ace), and whether or not the <b>player</b> holds a usable ace (0 or 1) or (<code>True or False</code>).\n",
    "<p><code>Tuple(the players current sum,dealer's one showing card player holds, usable ace (0 or 1) )</code></p>\n",
    "We can we can view the space using the method <code>observation_space</code>\n",
    "\n",
    "- The highest score you can achieve is (11,10,11), and the lowest is (1), so there are 32 states for the player's score.\n",
    "- The dealer only shows one card, which can be anything from 1 to 11\n",
    "- The 'usable ace' space is True/False, so the size of the space is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reset the environment  and observe the first state $S_{0}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=environment.reset()\n",
    "print(\"s_{}={}\".format(0, environment.observation_space.sample())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out some random states $S_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(\"s_{}={}\".format(i, environment.observation_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also see the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can randomly generate an action 0 for stay and 1 for hit .\n",
    "<p>stay $A_t= 0$ </p>\n",
    "<p>hit $A_t= 1$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    action=environment.action_space.sample()\n",
    "    if action:\n",
    "        print(\"Hit, A_{}={}\".format(t,action))\n",
    "    else:\n",
    "        print(\"Stay\",action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also see the dealers cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.dealer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(environment.observation_space.sample())\n",
    "print(environment.player)\n",
    "print(environment.dealer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"RP\">Play BlackJack With Random Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random actions and play some  games and calculate the return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=10\n",
    "sum_=0\n",
    "result=0\n",
    "error=0\n",
    "\n",
    "#setting the seed for reproduceability\n",
    "environment.seed(0)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state=environment.reset()\n",
    "    done=False\n",
    "    print(\"_________________________________________\")\n",
    "    print(\"episode {}\".format(episode))\n",
    "   \n",
    "\n",
    "    print(\"state:\",state)\n",
    "    print(\"player has\", environment.player)\n",
    "    print(\"the players current sum: {},\\n  dealer's one showing card: {}, usable ace: {}\".format(state[0],state[1],state[2]))\n",
    "    print('dealer', environment.dealer)\n",
    "    while (not(done)):\n",
    "    \n",
    "        action = environment.action_space.sample()\n",
    "       \n",
    "        if action:\n",
    "            print(\"hit\")\n",
    "        else:\n",
    "            print(\"stay\")\n",
    "        \n",
    "        print(\"action:\",action)\n",
    "\n",
    "        state,reward,done,info = environment.step(action)\n",
    "    print(\"Done :\", done)\n",
    "    result=game_result(environment,state)\n",
    "    sum_+=reward\n",
    "sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"YPE\"> You Play Environment</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You play the black jack Play Environment and calculate return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "your_return = 0\n",
    "for episode in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    print(\"_________________________________________\")\n",
    "    print(\"episode {}\".format(episode))\n",
    "    print(\"You are. the agent\")\n",
    "\n",
    "    print(\"player has\", environment.player)\n",
    "    print(\"the players current sum: {},\\n  dealer's one showing card: {}, usable ace: {}\".format(state[0],state[1],state[2]))\n",
    "    print('dealer', environment.dealer)\n",
    "    print(\"the players current sum: {},dealer's one showing card: {}, usable ace: {}\".format(state[0],state[1],state[2]))\n",
    "    while (not(done)):\n",
    "    \n",
    "        action=int(input(\"Press one to Hit and Zero to stay\"))\n",
    "        if action:\n",
    "            print(\"hit\")\n",
    "\n",
    "        else:\n",
    "            print(\"stay\")\n",
    "        \n",
    "        print(\"action:\",action)\n",
    "\n",
    "        state,reward,done,info=environment.step(action)\n",
    "        print(state)\n",
    "    result=game_result(environment,state)\n",
    "    your_return += reward\n",
    "print(\"your return\",your_return )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>About the Authors:</b> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>References</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol type=\"1\">\n",
    "    <li>Sutton, Richard S., and Andrew G. Barto. \"Reinforcement learning: An introduction.\" 2011</li>\n",
    "    <li><a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py\">blackjack openai Gym</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
